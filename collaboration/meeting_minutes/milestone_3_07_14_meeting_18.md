# üìã Meeting Minutes #18 ‚Äì Dataset Concerns, Relevance & Strategy Forward

**Date:** Monday, July 14, 2025  
**Time:** 09:30 AM ‚Äì 11:45 AM EST  
**Meeting Link:** [Google Meet](https://meet.google.com/thw-hymo-eah)  
**Datasets Sheet (Master):** [View Sheet](https://docs.google.com/spreadsheets/d/1zCrW4jRR-sTfQ-EHWYs6SaTcZx6gNuY0mocGLPuhJFw/edit#gid=0)

**Attendees:**  

- Jola-Moses  
- Karim Makie  
- Muqadsa Tahir  
- Omer Dafaalla  
- Robel Mengsteab  

---

## Meeting Purpose & Rationale

To align on how we assess dataset quality, triage for analysis (manual/NLP),
and move forward with a shared understanding of what makes data relevant ‚Äî
especially under constraints like limited direct data, proxy documents, and
systemic exclusion patterns that aren‚Äôt always keyword-heavy.

---

## Discussion & Outcomes

### 1. Reframing Dataset Review

- We acknowledged real concerns around:
  - Limited firsthand data from disabled entrepreneurs in SSA.
  - Prevalence of global or general sources.
  - Sparse, shallow, or promotional content (e.g., company homepages).

- Relevance isn‚Äôt about keywords alone ‚Äî it‚Äôs about whether a dataset **helps
  surface real patterns of digital exclusion** in the context of our RQ.

- We agreed that each dataset needs to be viewed **in relation to the README,
  Background Review, and Iceberg Model**, not in isolation.

---

### 2. Quality & Triage Clarity

We formalized a 4-tier categorization model to help streamline our review process:

<!-- markdownlint-disable MD013 -->
| Category     | Use Case                                                                 |
|--------------|--------------------------------------------------------------------------|
| **Manual**   | Narrative-rich/lived experience and aligned with RQ suited for in-depth coding.              |
| **NLP-only** | Thematic breadth, even if shallow; useful for clustering or topic models.|
| **Contextual** | Provides framing/background; usable with clear caveats.                 |
| **Exclude**  | Too shallow, off-topic, low-signal, or heavily processed to yield insight.  |
<!-- markdownlint-enable MD013 -->

> - Each dataset will be reviewed with this logic and cross-checked
> collaboratively in both the sheet and repo for consistency and transparency.
> - Some entries will need **relabeling or re-review** where past tags don‚Äôt align.

---

### 3. Shared Understanding of "Quality"

- Team interpretations of quality varied ‚Äî we agreed to review edge cases
  together to **build shared clarity**.
- ‚ÄúHigh-quality‚Äù = **narrative richness**, **strong PWD/SSA relevance**,
  **clear ties to digital exclusion**, and **thematic substance**.

---

## Live Dataset Review During Meeting

- We **walked through a few example datasets** and flagged:
  - Mismatches between titles and descriptions
  - Potential duplicates across and within theme files (e.g. identical articles under
  different IDs)
  - Inclusion of minimal-content or tokenistic sources
- Discussed thresholds for **narrative richness** and when sources may still
  offer **thematic scaffolding** via NLP.
- Agreed to **systematically flag questionable entries** for re-review with comments.

---

## Action Points

<!-- markdownlint-disable MD013 -->

| Task                                                                                           | Assigned To        | Due           |
|------------------------------------------------------------------------------------------------|--------------------|---------------|
| Begin collaborative re-review of all datasets (repo + sheet) using new 4-tier model                          | All team members   | July 15, 2025 |
| Cross-check for duplicates, title/description mismatches, and flag inconsistencies            | During re-review   | July 15, 2025 |
| Pair up where needed to ensure flagging decisions are sound                                   | All team members   | Rolling       |
| Re-align on inclusion criteria for manual sample (narrative, RQ fit, region, theme depth)                                     | Jola, Muqadsa,  Omer | July 16, 2025 |
| Investigate techniques to reduce NLP noise from short/shallow datasets                                                  | Karim, Omnia, Robel    | July 17, 2025 |
| Document triage model in methodology section and add triage notes/labels to spreadsheet to reflect decisions                        | All team members   | Soon |
<!-- markdownlint-enable MD013 -->

## Reflection

- Relevance is not binary‚Äîcontextual pieces can still be useful with proper
  framing.  
- Scarcity is real ‚Äî but **clarity in criteria and transparency in reasoning
  reduce confusion**.
- As relative newcomers to both NLP and qualitative methods, we recognize that
  working with imperfect or messy data is expected ‚Äî the key is to stay
  collaborative, honest, and thoughtful in navigating trade-offs.

---

## Next Steps

- Continue full dataset re-review for the next two days.
- Resolve all discrepancies in tagging or fit across the sheet and repo.
- Keep each other in the loop and co-review edge cases where decisions feel subjective.
- Explore **minimal targeted sourcing only if necessary** ‚Äî focus remains on
  sharpening what we have.

---

> *"We‚Äôre not just cleaning data‚Äîwe‚Äôre building the logic that gives our
> analysis integrity. With clarity and flexibility, we move forward to build
> something that matters."*

---

**Labels:** `meeting-minutes`, `data-analysis`, `dataset-triage`,
`milestone-3`  

**Related Issues:**  
[#132 ‚Äì Agenda: Dataset Concerns, Relevance &
Strategy Forward](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-24-repo/issues/132)  
[#125 ‚Äì Manual Coding
Workflow](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-24-repo/issues/125)  
[#127 ‚Äì NLP
Workflow](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-24-repo/issues/127)  
[#119 ‚Äì Parent Meetings
Thread](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-24-repo/issues/119)  
