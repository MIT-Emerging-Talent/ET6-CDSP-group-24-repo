# ðŸ“˜ Technical Description: Manual Coding  

## Overview

Our manual analysis sub-team (**Jola-Moses**, **Muqadsa Tahir**,
**Omer Dafaalla**) conducted thematic coding on **7 strategically selected
documents** (roughly 20% of the larger pool), focusing on *digital inclusion
and exclusion among self-employed (or entrepreneurs) with disabilities in
Sub-Saharan Africa (SSA)*.

This process followed a **structured qualitative workflow** aimed at surfacing
nuanced patterns of exclusion, identifying systemic and infrastructural
barriers, and understanding how digital technologies either support or hinder
meaningful access.

> This work complements the parallel NLP-based analysis conducted by the
> other sub-team, grounding our findings in rich human narratives and lived
> experienceâ€”elements often difficult to capture through automation alone.

---

## Approach

### 1. Coding Methodology

We used a **hybrid deductiveâ€“inductive thematic coding** approach:

- **Deductive**: We started with a structured codebook based on our
  research question, background frameworks (e.g., *iceberg model*, *digital
  inclusion literature*), and early dataset reviews

- **Inductive**: During calibration and coding, new themes and refinements
  emerged from the data  *(e.g., Private Sector Role, Intersectionality)* â€”
  which we incorporated back into the codebook.

> The final codebook included **six core themes** (e.g., *Access Barriers*,
> *Digital Technology & Design*) with targeted sub-codes.

---

### 2. Sample Selection

From a total of **39 reviewed datasets**, we strategically selected **7** for
in-depth manual coding. Selection was guided by the following criteria:

- Presence of *disability-specific narratives* or *clear themes of exclusion*
- Relevance to *digital tools, entrepreneurship,* or *systemic access issues*
- Regional focus on **Sub-Saharan Africa (SSA)**, including:
  - Direct references to SSA countries (e.g., Kenya, Nigeria), or
  - Global insights with clear *transferability* to SSA contexts
- Diversity of **source types and formats** (e.g., NGO reports, journalistic
  pieces, case studies)

<!-- markdownlint-disable MD013 -->
This approach ensured a balance between **regional specificity** and **broader
systemic relevance**. The selection process also surfaced early workflow gaps
(e.g., duplicate or misaligned datasets), which we addressed through
cross-referencing metadata, collaborative review and issue tracking before
finalizing our sample.
  *See: [`#138 Adding New Datasets`](https://github.com/MIT-Emerging-Talent/ET6-CDSP-group-24-repo/issues/138)*
<!-- markdownlint-enable MD013 -->

---

### 3. Intercoder Calibration & Process

Due to tight timelines, we adopted a **pragmatic yet rigorous** approach to calibration.

#### Calibration Session

- A live coding session with all 3 sub-team members
- Joint application of the **draft codebook** to 2-3 shared documents
- Clarified ambiguous codes and surfaced interpretive differences

**Key Steps:**

1. **Codebook Alignment**  
   â†’ Clarified definitions, *"When to Apply"* rules, and edge cases

2. **Reconciliation**  
   â†’ Reviewed disagreements and refined distinctions  
   *(e.g., `Digital Skills Gap â€“ Digital Literacy` vs. `Training Needs`;
   `Systemic Barriers` vs. `Invisible Barriers`)*

3. **Codebook Revision**  
   â†’ Finalized codebook with improved definitions and clearer application guidelines

> While we didnâ€™t hold additional group review meetings post-coding (due to
> deadlines), our **shared notes system** and **flagged segments** provided a
> lightweight consistency check.

---

### 4. Individual Coding

After calibration, the 7 documents were **split and coded individually** using
the finalized codebook. Coding was centralized in a **Google Sheet** with
structured fields:

- `Text Excerpt`
- `Assigned Codes / Sub-Codes`
- `Interpretive Notes or Memos`
- `Flags for New Sub-Codes or Ambiguities`

> This structure enabled *traceability* and streamlined the **review &
> synthesis phase**.

**Access the Manual Coding Sheet:**
For full transparency and to examine the raw codes applied during our manual
analysis, please refer to our collaborative Google Sheet:
**[Manual Coding Sheet (Google Sheet)](https://docs.google.com/spreadsheets/d/1ttROjrY1YECIfhm5oz4luWHxWq_MTShfQBsiFP1Pnvg/edit?gid=894372809#gid=894372809)**

---

### 5. Cross-Review & Reconciliation

Due to time constraints, one team member (Jola) took the lead on the final
**cleaning and reconciliation** of the coding sheet.

**Finalization Tasks:**

- Reviewed flagged sections and applied emerging sub-codes
- Checked consistency across all coded segments
- Prepared memos and synthesis notes for integration with NLP findings

> While we didnâ€™t do full post-coding review meetings, consistency was upheld
> through **early alignment**, **centralized notes**, and a **shared codebook**.

---

## Why Manual Coding?

Manual coding gave us the **interpretive depth** needed to meaningfully
complement the NLP analysis.  
Manual coding was a deliberate choice for this subset of 7 documents, offering
deep, context-sensitive insights that complement the scale-oriented strengths of
NLP. This approach allowed us to:

- Identify *subtle patterns of exclusion*â€”from emotional tone to implicit
  structural barriers
- Engage deeply with *rich narratives* and *underrepresented voices*
- Build a grounded thematic framework to interpret NLP outputs

---

## Limitations & Alternatives

### Known Limitations

- **Sample Size**: Only 7 documents were coded, which limits generalizability
  across all 39.
- **Selection Bias**: Prioritized clarity and accessibilityâ€”may underrepresent
  informal or multimedia content
- **Intercoder Variability**: While we aligned through calibration and a shared
  codebook, asynchronous workflows meant full post-coding reconciliation wasn't
  feasible. Some variation in interpretation may persist.
- **Scalability**: Manual coding is **resource-intensive** and doesnâ€™t scale to
  larger datasets

---

### Complementary & Alternative Approaches

Our project used a **mixed-methods approach**, combining manual coding with NLP
techniques to balance depth and scale.

- **NLP Techniques**  
  â†’ The other sub-team applied topic modeling and keyword extraction across the
  full dataset to surface scalable patterns.

- **QDA Software (e.g., NVivo, ATLAS.ti)**  
  â†’ Could support future work with better code tracking, visuals, and
  inter-coder stats more efficiently.

- **Semi-Supervised NLP Models**  
  â†’ Training a model on our coded sample could help scale interpretation while
  preserving nuance

- **Mixed Methods Triangulation**  
  â†’ Combining survey data, NLP, and qualitative coding could enrich both the
  thematic depth and generalizability of results.

---

## Final Reflection

Manual codingâ€”though methodical and time-consumingâ€”served as the
**interpretive backbone** of our milestone.

> It allowed us to surface the *lived realities* of exclusion and access in
> ways automated methods alone might miss.  
> This process helped ensure our project remained **grounded in people, context,
> and nuance**.

---
